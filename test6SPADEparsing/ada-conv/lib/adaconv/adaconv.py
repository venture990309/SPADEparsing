from math import ceil, floor

import torch
from torch import nn
from torch.nn import functional as F

class AdaConv2d(nn.Module):
    """
    Implementation of the Adaptive Convolution block. Performs a depthwise seperable adaptive convolution on its input X.
    The weights for the adaptive convolutions are generated by a KernelPredictor module based on the style embedding W.
    The adaptive convolution is followed by a normal convolution.

    References:
        https://openaccess.thecvf.com/content/CVPR2021/papers/Chandran_Adaptive_Convolutions_for_Structure-Aware_Style_Transfer_CVPR_2021_paper.pdf


    Args:
        in_channels: Number of channels in the input image.
        out_channels: Number of channels produced by final convolution.
        kernel_size: The kernel size of the final convolution.
        n_groups: The number of groups for the adaptive convolutions.
            Defaults to 1 group per channel if None.

    Input shape:
        x: Input tensor.
        w_spatial: Weights for the spatial adaptive convolution.
        w_pointwise: Weights for the pointwise adaptive convolution.
        bias: Bias for the pointwise adaptive convolution.
    """

    def __init__(self, in_channels, out_channels, kernel_size=3):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        # self.att = Self_Attn(512, nn.ReLU())

        padding = (kernel_size - 1) / 2
        self.conv = nn.Conv2d(in_channels=in_channels,
                              out_channels=out_channels,
                              kernel_size=(kernel_size, kernel_size),
                              padding=(ceil(padding), floor(padding)),
                              padding_mode='reflect')
        self.conv_mean0 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std0 = nn.Conv2d(in_channels, in_channels, 1)

        self.conv_mean1 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std1 = nn.Conv2d(in_channels, in_channels, 1)

        self.conv_mean2 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std2 = nn.Conv2d(in_channels, in_channels, 1)

        self.conv_mean3 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std3 = nn.Conv2d(in_channels, in_channels, 1)

        self.conv_mean4 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std4 = nn.Conv2d(in_channels, in_channels, 1)

        self.conv_mean5 = nn.Conv2d(in_channels, in_channels, 1)
        self.conv_std5 = nn.Conv2d(in_channels, in_channels, 1)

        self.noise_var = nn.Parameter(torch.zeros(in_channels), requires_grad=True)

    def forward(self, x, w_spatial, w_pointwise, bias, j):
        added_noise = (torch.randn(x.shape[0], x.shape[3], x.shape[2], 1).cuda() * self.noise_var).transpose(1, 3)
        x = F.instance_norm(x)

        # F.conv2d does not work with batched filters (as far as I can tell)...
        # Hack for inputs with > 1 sample
        ys = []
        for i in range(len(x)):
            y = self._forward_single(x[i:i + 1], w_spatial, w_pointwise, bias, j)
            ys.append(y)
        ys = torch.cat(ys, dim=0)
        # ys = self.att(ys)

        ys = self.conv(ys)
        return ys

    def _forward_single(self, x, w_spatial, w_pointwise, bias, j):
        if x.shape[1] == 512:
            scale = self.__getattr__('conv_mean' + str(j))(w_spatial[0])
            bias = self.__getattr__('conv_std' + str(j))(w_spatial[0])
            x = (x * scale) + bias
        elif x.shape[1] == 256:
            scale = self.__getattr__('conv_mean' + str(j))(w_spatial[1])
            bias = self.__getattr__('conv_std' + str(j))(w_spatial[1])
            x = (x * scale) + bias
        elif x.shape[1] == 128:
            scale = self.__getattr__('conv_mean' + str(j))(w_spatial[2])
            bias = self.__getattr__('conv_std' + str(j))(w_spatial[2])
            x = (x * scale) + bias
        elif x.shape[1] == 64:
            scale = self.__getattr__('conv_mean' + str(j))(w_spatial[3])
            bias = self.__getattr__('conv_std' + str(j))(w_spatial[3])
            x = (x * scale) + bias

        # if j == 2 or j == 3:
        #     x = F.pad(x, pad=pad, mode='reflect')
        #     x = F.conv2d(x, w_spatial, groups=self.n_groups)
        # else:
        #     x = F.conv2d(x, w_pointwise, groups=self.n_groups, bias=bias)
        # x = F.conv2d(x, w_spatial, groups=self.n_groups)
        # x = F.conv2d(x, w_pointwise, groups=self.n_groups, bias=bias)
        return x
